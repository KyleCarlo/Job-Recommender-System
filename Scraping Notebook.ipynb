{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2db9e60-1c6a-40a8-8d5d-416368957326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e98529-f1b5-4d11-a080-e4912c60a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a2be0-f600-46ad-8b7c-b4246abaa039",
   "metadata": {},
   "source": [
    "# Scraping Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ac0e08-be70-4d8d-b574-ed4d1a5af5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\"\n",
    "len_jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462a4870-0a96-4333-9dcc-d9fa007700d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 117.53085374832153\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://ph.linkedin.com/jobs/{job_query}-jobs\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "    job_list = driver.find_elements(By.XPATH, \"//ul[@class='jobs-search__results-list']/li\")\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        descs = []\n",
    "        try:\n",
    "            job_title = job.find_element(By.XPATH, \".//h3[@class='base-search-card__title']\").get_attribute('innerHTML').strip()\n",
    "            card = job.find_element(By.XPATH, \".//a[@data-tracking-control-name='public_jobs_jserp-result_search-card']\")\n",
    "            link = card.get_attribute('href')\n",
    "            descs = [job_title, link]\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            location = job.find_element(By.XPATH, \".//span[@class='job-search-card__location']\").get_attribute('innerHTML')\n",
    "            location = location.replace('\\n', '').strip()\n",
    "            descs.append(location)\n",
    "        except:\n",
    "            descs.append('')\n",
    "        \n",
    "        try: \n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']/a\").get_attribute('innerHTML').strip()\n",
    "            descs.append(company)\n",
    "        except:\n",
    "            try:\n",
    "                company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']\").get_attribute('innerHTML').strip()\n",
    "                descs.append(company)\n",
    "            except:\n",
    "                descs.append('')\n",
    "                \n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, descs)\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "\n",
    "        # limit to max len_jobs only\n",
    "        if len(jobs_scraped) >= len_jobs:\n",
    "            break\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if jobs_scraped.shape == (4,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "        \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        descs = []\n",
    "        try:\n",
    "            # scraping emp_type, job_func, job_desc, posted ago\n",
    "            driver.get(job[1])\n",
    "            wait = WebDriverWait(driver, timeout=2)\n",
    "            desc_job = wait.until(EC.presence_of_all_elements_located((By.XPATH, \".//ul[@class='description__job-criteria-list']/li\")))\n",
    "            descs = [job[1]]\n",
    "            for i in [1,2]:\n",
    "                try:\n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//span\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                    descs.append(detail)\n",
    "                except:\n",
    "                    descs.append('')\n",
    "            try:\n",
    "                desc_gen = driver.find_element(By.XPATH, \"//div[@class='description__text description__text--rich']/section/div\")\n",
    "                desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "                descs.append(desc_gen)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                posted_ago = driver.find_element(By.XPATH, \"//span[@class='posted-time-ago__text topcard__flavor--metadata']\")\n",
    "                posted_ago = posted_ago.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(posted_ago)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        except: \n",
    "            descs = [job[3],'','','','']\n",
    "        if len(job_descs) == 0:\n",
    "            job_descs = np.append(job_descs, descs)\n",
    "        else:\n",
    "            job_descs = np.vstack([job_descs, descs])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # merging\n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title','link', 'location', 'company']\n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if job_descs_df.shape == (5,1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    \n",
    "    job_descs_df.columns = ['link','emp_type', 'job_func', 'job_desc', 'posted']\n",
    "    linkedin_df = jobs_df.merge(job_descs_df, on='link', how='left')\n",
    "except:\n",
    "    print(\"Unable to Scrape Linkedin\")\n",
    "finally:\n",
    "    # close driver\n",
    "    try:\n",
    "        driver.close()\n",
    "    except:\n",
    "        print(\"Unable to Scrape Linkedin\")\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c28a1-cfb7-40e6-9b15-e2a2cf3bf3a2",
   "metadata": {},
   "source": [
    "# Scraping FoundIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13ae83b-2953-4c67-8f68-46665d22b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\"\n",
    "len_jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba34f8e9-0d28-4f33-951b-a3b8b8dd3d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 151.69784998893738\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://www.foundit.com.ph/search/{job_query}-jobs\"\n",
    "\n",
    "try:\n",
    "    jobs_scraped = np.array([])\n",
    "    page_num = 1\n",
    "    while True:\n",
    "        driver.get(f\"{url}{f\"-{page_num}\" if page_num > 1 else ''}\")\n",
    "        job_list = driver.find_elements(By.XPATH, \"//div[@class='srpResultCard']/div\")\n",
    "        if len(job_list) == 0:\n",
    "            break\n",
    "        job_list = job_list[1:] # remove the header\n",
    "        for job in job_list:\n",
    "            # scraping title, company, url\n",
    "            try:\n",
    "                job_title = job.find_element(By.XPATH, \".//a[@title]\")\n",
    "                link = job_title.get_attribute('href')\n",
    "                job_title = job_title.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                company = job.find_element(By.XPATH, \".//div[@class='companyName']/span\")\n",
    "                company = company.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            except: \n",
    "                company = ''\n",
    "            descs = [job_title, company, link]\n",
    "        \n",
    "            # scraping location, posted ago\n",
    "            job.find_element(By.XPATH, './/div[@onclick]/div').click()\n",
    "            wait = WebDriverWait(driver, 5)\n",
    "            try:\n",
    "                desc_job = wait.until(EC.presence_of_all_elements_located((By.XPATH, \".//div[@id='jobHighlight']/div/div/div\")))\n",
    "                for i in [0,2]:\n",
    "                    if i != 2:\n",
    "                        detail = desc_job[i].find_element(By.XPATH, \".//div[@class='details']\")\n",
    "                        detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                    else: \n",
    "                        detail = desc_job[i].find_element(By.XPATH, \".//span[@class='btnHighighlights']\")\n",
    "                        detail = detail.get_attribute('innerHTML').replace('\\n', '').split('</i>')\n",
    "                        detail = detail[1].strip()\n",
    "                    descs.append(detail)\n",
    "            except:\n",
    "                descs.append('','','')\n",
    "        \n",
    "            # scraping emp_type, job function, general job_desc\n",
    "            try:\n",
    "                desc_job_2 = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobDetail']/div/div\")))\n",
    "                for i in [0,2]:\n",
    "                    try:\n",
    "                        detail = desc_job_2[i].find_element(By.XPATH, \".//div[@class='jobDesc']\")\n",
    "                        detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                        descs.append(detail)\n",
    "                    except:\n",
    "                        descs.append('')\n",
    "            except:\n",
    "                descs.extend(['',''])\n",
    "            try:\n",
    "                desc_gen = wait.until(EC.visibility_of_element_located((By.XPATH, \".//p[@class='jobDescInfo']\")))\n",
    "                desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "                descs.append(desc_gen)\n",
    "            except:\n",
    "                descs.append('')\n",
    "                \n",
    "            if len(jobs_scraped) == 0:\n",
    "                jobs_scraped = np.append(jobs_scraped, descs)\n",
    "            else:\n",
    "                jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "\n",
    "        # limit to max len_jobs only\n",
    "        if len(jobs_scraped) >= len_jobs:\n",
    "            break\n",
    "        page_num += 1\n",
    "\n",
    "    # converting to DataFrame\n",
    "    if jobs_scraped.shape == (8,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    foundit_df = pd.DataFrame(jobs_scraped)\n",
    "    foundit_df.columns = ['title','company','link','location','posted','emp_type','job_func','job_desc']\n",
    "except:\n",
    "    print(\"Unable to Scrape Foundit\")\n",
    "finally:\n",
    "    # close driver\n",
    "    try:\n",
    "        driver.close()\n",
    "    except:\n",
    "        print(\"Unable to Scrape Foundit\")\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9a86-55f2-45c6-bb76-5a1aa55d8ae2",
   "metadata": {},
   "source": [
    "# Scraping Jobstreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8a48313-054e-4a3a-b682-b2e276eb4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\"\n",
    "len_jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "474f00ec-50b2-4afb-8dd7-96408ad51104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 729.1062438488007\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://ph.jobstreet.com/{job_query}-jobs\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    while True:\n",
    "        try:\n",
    "            job_list = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//article[@data-automation='normalJob']\")))\n",
    "            for job in job_list:\n",
    "                descs = []\n",
    "                # scraping title, company, link\n",
    "                try:\n",
    "                    job_title = job.find_element(By.XPATH, \".//a[@data-automation='jobTitle']\")\n",
    "                    link = job_title.get_attribute('href')\n",
    "                    job_title = job_title.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                    company = job.find_element(By.XPATH, \".//a[@data-type='company']\")\n",
    "                    company = company.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                    descs.append(job_title)\n",
    "                    descs.append(link)\n",
    "                    descs.append(company)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    posted_ago = job.find_element(By.XPATH, \".//span[@data-automation='jobListingDate']\")\n",
    "                    posted_ago = posted_ago.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                    descs.append(posted_ago)\n",
    "                except:\n",
    "                    descs.append('')\n",
    "                \n",
    "                if len(jobs_scraped) == 0:\n",
    "                    jobs_scraped = np.append(jobs_scraped, descs)\n",
    "                else:\n",
    "                    jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "                    \n",
    "            # limit to max len_jobs only\n",
    "            if len(jobs_scraped) >= len_jobs:\n",
    "                jobs_scraped = jobs_scraped[:len_jobs]\n",
    "                break\n",
    "                \n",
    "            next_button = driver.find_element(By.XPATH, \".//a[@aria-label='Next']\")\n",
    "            next_button.click()\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    if jobs_scraped.shape == (4,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        try:\n",
    "            driver.get(job[1])\n",
    "            descs = [job[1]]\n",
    "            try:\n",
    "                location = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-location']/a\")\n",
    "                location = location.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(location)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                job_func = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-classifications']/a\")\n",
    "                job_func = job_func.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(job_func)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                emp_type = driver.find_element(By.XPATH, \"//span[@data-automation='job-detail-work-type']/a\")\n",
    "                emp_type = emp_type.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(emp_type)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                gen_desc = driver.find_element(By.XPATH, \"//div[@data-automation='jobAdDetails']/div\")\n",
    "                gen_desc = gen_desc.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(gen_desc)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        except:\n",
    "            descs = [job[2], '', '', '', '']\n",
    "        if len(job_descs) == 0:\n",
    "            job_descs = np.append(job_descs, descs)\n",
    "        else:\n",
    "            job_descs = np.vstack([job_descs, descs])\n",
    "    \n",
    "    # merging\n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title', 'link', 'company', 'posted']\n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if job_descs_df.shape == (5,1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    \n",
    "    job_descs_df.columns = ['link', 'location', 'job_func', 'emp_type', 'job_desc']\n",
    "    jobstreet_df = jobs_df.merge(job_descs_df, on='link', how='left')\n",
    "except:\n",
    "    print(\"Unable to Scrape Jobstreet\")\n",
    "finally:\n",
    "    # close the driver\n",
    "    try:\n",
    "        driver.close()\n",
    "    except:\n",
    "        print(\"Unable to Scrape Jobstreet\")\n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46d883-df6d-4f48-94da-7114c5b65b4e",
   "metadata": {},
   "source": [
    "# Scraping Kalibrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dffd237-1fcf-4c4b-b8b0-d8b2fd012667",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\"\n",
    "len_jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cf39573-3825-4930-bfbe-261ad806650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 110.56875658035278\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://www.kalibrr.com/home/te/{job_query}\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    job_list = []\n",
    "    # load job list \n",
    "    while len(job_list) < len_jobs:\n",
    "        # click button to load more jobs until job_list > len_jobs\n",
    "        try:\n",
    "            # Re-evaluate job_list after each load\n",
    "            wait = WebDriverWait(driver, 5)\n",
    "            job_list = wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='k-font-dm-sans k-rounded-lg k-bg-white k-border-solid k-border hover:k-border-2 hover:k-border-primary-color k-border k-group k-flex k-flex-col k-justify-between css-1otdiuc']\")))\n",
    "            \n",
    "            load_more = wait.until(EC.presence_of_element_located((By.XPATH, \"//button[@class='k-btn-primary']\")))\n",
    "            load_more.click()\n",
    "        except:\n",
    "            break\n",
    "    job_list[:len_jobs]\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    i = 0\n",
    "    for job in job_list:\n",
    "        i += 1\n",
    "        descs = []\n",
    "        # scraping job title, company, emp_type, location\n",
    "        try:\n",
    "            job_title = job.find_element(By.XPATH, \".//h2[@data-tooltip-id='job-title-tooltip-[object Object]']/a\")\n",
    "            url = job_title.get_attribute('href')\n",
    "            job_title = job_title.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            descs.append(job_title)\n",
    "            descs.append(url)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            company = job.find_element(By.XPATH, \".//span[@class='k-inline-flex k-items-center k-mb-1']/a\")\n",
    "            company = company.text.replace('\\n', '').strip()\n",
    "            descs.append(company)\n",
    "        except:\n",
    "            descs.append('')\n",
    "            \n",
    "        try:\n",
    "            emp_type = job.find_element(By.XPATH, \"./div[@class='k-relative']/div/span/span[@class='k-text-gray-500']\")\n",
    "            emp_type = emp_type.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            descs.append(emp_type)\n",
    "        except:\n",
    "            descs.append('')\n",
    "    \n",
    "        try:\n",
    "            location = job.find_element(By.XPATH, \"./div[@class='k-relative']/div/span/span[@class='k-text-gray-500 k-block k-pointer-events-none']\")\n",
    "            location = location.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            descs.append(location)\n",
    "        except:\n",
    "            descs.append('')\n",
    "            \n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, descs)\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "\n",
    "        # limit to len_jobs only\n",
    "        if len(jobs_scraped) >= len_jobs:\n",
    "            break\n",
    "    \n",
    "    if jobs_scraped.shape == (5,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        descs = [job[1]]\n",
    "        try:\n",
    "            driver.get(job[1])\n",
    "            # scrape job_func, posted\n",
    "            try:\n",
    "                wait = WebDriverWait(driver, timeout=2)\n",
    "                job_func = wait.until(EC.presence_of_element_located((By.XPATH, \".//div[@class='md:k-flex']//dt[contains(text(),'Job Category')]/following-sibling::dd/a\")))\n",
    "                job_func = job_func.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                descs.append(job_func)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                posted = driver.find_element(By.XPATH, \".//div[@class='k-text-subdued k-text-caption md:k-text-right md:k-absolute md:k-right-0 md:k-top-0 md:k-p-4']/p\")\n",
    "                posted = posted.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                descs.append(posted)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            # scrape desc\n",
    "            gen_desc = ''\n",
    "            try:\n",
    "                job_desc = driver.find_element(By.XPATH, \".//div[@itemprop='description']\")\n",
    "                job_desc = job_desc.get_attribute('innerHTML')\n",
    "                gen_desc += job_desc\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "            try:\n",
    "                job_qual = driver.find_element(By.XPATH, \".//div[@itemprop='qualifications']\")\n",
    "                job_qual = job_qual.get_attribute('innerHTML')\n",
    "                gen_desc += job_qual\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "            try:\n",
    "                job_benef = driver.find_element(By.XPATH, \".//div[@itemprop='jobBenefits']\")\n",
    "                job_benef = job_benef.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                gen_desc += job_qual\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "            try:\n",
    "                job_skills = driver.find_element(By.XPATH, \".//ul\")\n",
    "                job_skills = job_skills.get_attribute('innerHTML')\n",
    "                gen_desc += job_skills\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "        \n",
    "            descs.append(gen_desc)\n",
    "        \n",
    "            if len(job_descs) == 0:\n",
    "                job_descs = np.append(job_descs, descs)\n",
    "            else:\n",
    "                job_descs = np.vstack([job_descs, descs])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title', 'link' ,'company', 'emp_type', 'location']\n",
    "    \n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    if job_descs_df.shape == (3, 1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    job_descs_df.columns = ['link', 'job_func', 'posted', 'gen_desc']\n",
    "    kalibrr_df = jobs_df.merge(job_descs_df, on='link', how='left')\n",
    "\n",
    "except:\n",
    "    print(\"Unable to Scrape Kalibrr\")\n",
    "finally:\n",
    "    # close driver\n",
    "    try:\n",
    "        driver.close()\n",
    "    except:\n",
    "        print(\"Unable to Scrape Kalibrr\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1945ab3-830f-4cb0-8520-d9afc949fd14",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25255944-69bd-4d5f-be96-858b2b381884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "linkedin_df.to_csv('linkedin.csv')\n",
    "foundit_df.to_csv('foundit.csv')\n",
    "jobstreet_df.to_csv('jobstreet.csv')\n",
    "kalibrr_df.to_csv('kalibrr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ad4d7-e34e-4286-9219-d5b3dbe600a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "linkedin_df = pd.read_csv('linkedin.csv')\n",
    "foundit_df = pd.read_csv('foundit.csv')\n",
    "jobstreet_df = pd.read_csv('jobstreet.csv')\n",
    "kalibrr_df = pd.read_csv('kalibrr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cfde692-b079-4fa7-ab24-499c9353dce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7b96fe0-0bc1-45fb-87a8-a87fcf6ef89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4930d65-1f3a-46dc-8c71-c8ad2b234f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
