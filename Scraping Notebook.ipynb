{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db9e60-1c6a-40a8-8d5d-416368957326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e98529-f1b5-4d11-a080-e4912c60a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a2be0-f600-46ad-8b7c-b4246abaa039",
   "metadata": {},
   "source": [
    "# Scraping Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22ac0e08-be70-4d8d-b574-ed4d1a5af5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a4870-0a96-4333-9dcc-d9fa007700d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://ph.linkedin.com/jobs/{job_query}-jobs\"\n",
    "try:\n",
    "    driver.get(url)\n",
    "    ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "    job_list = driver.find_elements(By.XPATH, \"//ul[@class='jobs-search__results-list']/li\")\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        card = job.find_element(By.XPATH, \".//a[@data-tracking-control-name='public_jobs_jserp-result_search-card']\")\n",
    "        url = card.get_attribute('href')\n",
    "        location = job.find_element(By.XPATH, \".//span[@class='job-search-card__location']\").get_attribute('innerHTML')\n",
    "        location = location.replace('\\n', '').strip()\n",
    "        job_title = job.find_element(By.XPATH, \".//h3[@class='base-search-card__title']\").get_attribute('innerHTML').strip()\n",
    "        try: \n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']/a\").get_attribute('innerHTML').strip()\n",
    "        except:\n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']\").get_attribute('innerHTML').strip()\n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, [job_title, company, location, url])\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, [job_title, company, location, url]])\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if jobs_scraped.shape == (4,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "        \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        descs = []\n",
    "        try:\n",
    "            # scraping seniority, emp_type, job_func, job_desc, posted ago\n",
    "            driver.get(job[3])\n",
    "            wait = WebDriverWait(driver, timeout=2)\n",
    "            ul = wait.until(EC.presence_of_element_located((By.XPATH, \".//ul[@class='description__job-criteria-list']\")))\n",
    "            desc_job = ul.get_attribute('outerHTML')\n",
    "            desc_job = BeautifulSoup(desc_job, 'html.parser')\n",
    "            desc_job = desc_job.find_all('span')\n",
    "            descs = [job[3]]\n",
    "            for i in [0,1,2]:\n",
    "                try:\n",
    "                    descs.append(desc_job[i].contents[0].replace('\\n', '').strip())\n",
    "                except:\n",
    "                    descs.append('')\n",
    "            try:\n",
    "                desc_gen = driver.find_element(By.XPATH, \"//div[@class='description__text description__text--rich']/section/div\")\n",
    "                desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "                descs.append(desc_gen)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                posted_ago = driver.find_element(By.XPATH, \"//span[@class='posted-time-ago__text topcard__flavor--metadata']\")\n",
    "                posted_ago = posted_ago.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(posted_ago)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        except: \n",
    "            descs = ['','','','','','']\n",
    "            \n",
    "        if len(job_descs) == 0:\n",
    "            job_descs = np.append(job_descs, descs)\n",
    "        else:\n",
    "            job_descs = np.vstack([job_descs, descs])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # merging\n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title', 'company', 'location', 'link']\n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if job_descs_df.shape == (6,1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    \n",
    "    job_descs_df.columns = ['link','seniority','emp_type', 'job_function', 'job_desc', 'posted_ago']\n",
    "    linkedin_df = jobs_df.merge(job_descs_df, on='link')\n",
    "except:\n",
    "    print(\"LinkedIn not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    driver.close()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c28a1-cfb7-40e6-9b15-e2a2cf3bf3a2",
   "metadata": {},
   "source": [
    "# Scraping FoundIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f13ae83b-2953-4c67-8f68-46665d22b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba34f8e9-0d28-4f33-951b-a3b8b8dd3d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 29.352275133132935\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://www.foundit.com.ph/search/{job_query}-jobs\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    job_list = driver.find_elements(By.XPATH, \"//div[@class='srpResultCard']/div\")\n",
    "    job_list = job_list[1:] # remove the header\n",
    "    \n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        # scraping title, company, url\n",
    "        job_title = job.find_element(By.XPATH, \".//a[@title]\")\n",
    "        url = job_title.get_attribute('href')\n",
    "        job_title = job_title.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "        company = job.find_element(By.XPATH, \".//div[@class='companyName']/span\")\n",
    "        company = company.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "        descs = [job_title, company, url]\n",
    "    \n",
    "        # scraping location, seniority, posted ago\n",
    "        job.find_element(By.XPATH, './/div[@onclick]/div').click()\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        try:\n",
    "            desc_job = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobHighlight']/div/div/div\")))\n",
    "            for i in [0,1,2]:\n",
    "                if i != 2:\n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//div[@class='details']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                else: \n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//span[@class='btnHighighlights']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').split('</i>')\n",
    "                    detail = detail[1].strip()\n",
    "                descs.append(detail)\n",
    "        except:\n",
    "            descs.append('','','')\n",
    "    \n",
    "        # scraping emp_type, job function, general job_desc\n",
    "        try:\n",
    "            desc_job_2 = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobDetail']/div/div\")))\n",
    "            for i in [0,2]:\n",
    "                try:\n",
    "                    detail = desc_job_2[i].find_element(By.XPATH, \".//div[@class='jobDesc']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                    descs.append(detail)\n",
    "                except:\n",
    "                    descs.append('')\n",
    "        except:\n",
    "            descs.extend(['',''])\n",
    "        try:\n",
    "            desc_gen = wait.until(EC.visibility_of_element_located((By.XPATH, \".//p[@class='jobDescInfo']\")))\n",
    "            desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "            descs.append(desc_gen)\n",
    "        except:\n",
    "            descs.append('')\n",
    "            \n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, descs)\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "    \n",
    "    if jobs_scraped.shape == (9,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    foundit_df = pd.DataFrame(jobs_scraped)\n",
    "    foundit_df.columns = ['job_title','company','link','location','seniority','posted','emp_type','job_func','job_desc']\n",
    "except:\n",
    "    print(\"FoundIt not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    driver.close()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0b1aa-37be-46d8-88f3-f7d635d37958",
   "metadata": {},
   "source": [
    "# Scraping Kalibrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ab3c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"web-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428415d6-27a1-4842-8ab6-4e9126ffce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 5)\n",
      "['Web Developer' 'Full time' 'SEO Hacker' 'Parañaque, Philippines'\n",
      " 'https://www.kalibrr.com/c/seo-marketing-services-inc/jobs/240698/web-developer']\n",
      "(27, 4)\n",
      "['IT and Software' 'Associate / Supervisor'\n",
      " 'Develop custom WordPress themes from scratch, adhering to best practices and design standards. Utilize Custom Post Types and Advanced Custom Fields (ACF) plugins to enhance the functionality and flexibility of WordPress websites. Customize WooCommerce templates to meet specific project requirements. Collaborate with designers and other team members to ensure seamless integration of design and functionality. Troubleshoot and resolve technical issues related to WordPress development. Stay updated on the latest trends and technologies in WordPress development and implement them effectively. Maintain existing WordPress websites ensuring all functions are working and security measures are up to date. Writing well designed, testable, efficient code by using best software development practices. Experience in Shopify and other web platforms is a plus.'\n",
      " 'https://www.kalibrr.com/c/seo-marketing-services-inc/jobs/240698/web-developer']\n",
      "TOTAL TIME 163.1300504207611\n",
      "       job_title   job_type job_company            job_location  \\\n",
      "0  Web Developer  Full time  SEO Hacker  Parañaque, Philippines   \n",
      "\n",
      "                                             job_url        job_categ  \\\n",
      "0  https://www.kalibrr.com/c/seo-marketing-servic...  IT and Software   \n",
      "\n",
      "                job_level                                           job_desc  \n",
      "0  Associate / Supervisor  Develop custom WordPress themes from scratch, ...  \n"
     ]
    }
   ],
   "source": [
    "# insert scraping code here\n",
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://www.kalibrr.com/home/te/{job_query}\"\n",
    "\n",
    "kalibrr_df = pd.DataFrame()\n",
    "try:\n",
    "    driver.get(url)\n",
    "    job_list = driver.find_elements(By.XPATH, \"//div[@class='k-container k-grid k-grid-cols-1 md:k-grid-cols-2 xl:k-grid-cols-3 k-gap-4 k-mt-8 k-mb-10']/div\")\n",
    "    \n",
    "    # change all 27s to 100s if you want to scrape more jobs\n",
    "    # scrape title, type, company, location, url\n",
    "    jobs_scraped = np.array([])\n",
    "    while len(jobs_scraped) < 27:\n",
    "        # Re-evaluate job_list after each load\n",
    "        job_list = driver.find_elements(By.XPATH, \"//div[@class='k-container k-grid k-grid-cols-1 md:k-grid-cols-2 xl:k-grid-cols-3 k-gap-4 k-mt-8 k-mb-10']/div\")\n",
    "\n",
    "        for job in job_list:\n",
    "            if len(jobs_scraped) >= 27:\n",
    "                break  # Stop if 100 jobs are already scraped\n",
    "    \n",
    "            job_title_element = job.find_element(By.XPATH, \".//h2[@data-tooltip-id='job-title-tooltip-[object Object]']/a\")\n",
    "            job_title = job_title_element.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            job_url = job_title_element.get_attribute('href')\n",
    "\n",
    "            #print(job_title)\n",
    "\n",
    "            job_type = job.find_element(By.XPATH, \"./div[@class='k-relative']/div/span/span[@class='k-text-gray-500']\")\n",
    "            job_type = job_type.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            #print(job_type)\n",
    "\n",
    "            job_company = job.find_element(By.XPATH, \".//span[@class='k-inline-flex k-items-center k-mb-1']/a\")\n",
    "            job_company = job_company.text.strip()\n",
    "            #print(job_company)\n",
    "\n",
    "\n",
    "            job_location = job.find_element(By.XPATH, \"./div[@class='k-relative']/div/span/span[@class='k-text-gray-500 k-block k-pointer-events-none']\")\n",
    "            job_location = job_location.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            #print(job_location)\n",
    "\n",
    "            \n",
    "            #print(job_url)\n",
    "\n",
    "\n",
    "            if len(jobs_scraped) == 0:\n",
    "                jobs_scraped = np.append(jobs_scraped, [job_title, job_type ,job_company, job_location, job_url])\n",
    "            else:\n",
    "                jobs_scraped = np.vstack([jobs_scraped, [job_title, job_type,job_company, job_location, job_url]])\n",
    "        \n",
    "            #click button to load more jobs until jobs_scraped is 100 (NOT SURE IF IT SCRAPES THE NEWLY LOADED ONES)\n",
    "            if jobs_scraped.shape[0] < 27:\n",
    "                try:\n",
    "                    load_more = driver.find_element(By.XPATH, \"//button[@class='k-btn-primary']\")\n",
    "                    load_more.click()\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "    #print jobs scraped overview\n",
    "    print(jobs_scraped.shape)\n",
    "    print(jobs_scraped[0])\n",
    "\n",
    "    #scrape desc, job categ, job level\n",
    "    job_descs = []\n",
    "    for job in jobs_scraped:\n",
    "            descs = []\n",
    "            try:\n",
    "                driver.get(job[4])\n",
    "                wait = WebDriverWait(driver, timeout=2)\n",
    "                new_page = wait.until(EC.presence_of_element_located((By.XPATH, \".//div[@class='md:k-w-full md:k-pr-4 css-11e7ttb']\")))\n",
    "                job_all_desc = new_page.find_element(By.XPATH, \"./div[@itemprop='description']\")\n",
    "                all_texts = job_all_desc.find_elements(By.XPATH, \".//*[self::p or self::li or self::span or self::div or self::a]\")\n",
    "                job_desc = \" \".join([element.text for element in all_texts if element.text.strip() != \"\"])\n",
    "                #print(job_desc)\n",
    "\n",
    "                job_categ = new_page.find_element(By.XPATH, \".//div[@class='md:k-flex']//dt[contains(text(),'Job Category')]/following-sibling::dd/a\")\n",
    "                job_categ = job_categ.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "\n",
    "                job_level = new_page.find_element(By.XPATH, \".//div[@class='md:k-flex']//dt[contains(text(),'Job Level')]/following-sibling::dd/a\")\n",
    "                job_level = job_level.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "\n",
    "                descs.append(job_categ)\n",
    "                descs.append(job_level)\n",
    "                descs.append(job_desc)\n",
    "                job_url = job[4]\n",
    "                descs.append(job_url)\n",
    "            except:\n",
    "                descs = ['','','','']\n",
    "            \n",
    "            job_descs.append(descs)\n",
    "            time.sleep(2)\n",
    "    #check descs scraped overview\n",
    "    job_descs = np.array(job_descs)\n",
    "    print(job_descs.shape)\n",
    "    print(job_descs[0])\n",
    "\n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['job_title', 'job_type' ,'job_company', 'job_location', 'job_url']\n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    job_descs_df.columns = ['job_categ','job_level','job_desc','job_url']\n",
    "    kalibrr_df = jobs_df.merge(job_descs_df, on='job_url',how='left')\n",
    "\n",
    "except:\n",
    "    print(\"Kalibrr not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    driver.close()\n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)\n",
    "print(kalibrr_df.head(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9a86-55f2-45c6-bb76-5a1aa55d8ae2",
   "metadata": {},
   "source": [
    "# Scraping Jobstreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "383425ff-6874-46dc-aa1d-699166db2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert scraping code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
