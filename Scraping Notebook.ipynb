{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db9e60-1c6a-40a8-8d5d-416368957326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e98529-f1b5-4d11-a080-e4912c60a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a2be0-f600-46ad-8b7c-b4246abaa039",
   "metadata": {},
   "source": [
    "# Scraping Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22ac0e08-be70-4d8d-b574-ed4d1a5af5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a4870-0a96-4333-9dcc-d9fa007700d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://ph.linkedin.com/jobs/{job_query}-jobs\"\n",
    "try:\n",
    "    driver.get(url)\n",
    "    ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "    job_list = driver.find_elements(By.XPATH, \"//ul[@class='jobs-search__results-list']/li\")\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        card = job.find_element(By.XPATH, \".//a[@data-tracking-control-name='public_jobs_jserp-result_search-card']\")\n",
    "        url = card.get_attribute('href')\n",
    "        location = job.find_element(By.XPATH, \".//span[@class='job-search-card__location']\").get_attribute('innerHTML')\n",
    "        location = location.replace('\\n', '').strip()\n",
    "        job_title = job.find_element(By.XPATH, \".//h3[@class='base-search-card__title']\").get_attribute('innerHTML').strip()\n",
    "        try: \n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']/a\").get_attribute('innerHTML').strip()\n",
    "        except:\n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']\").get_attribute('innerHTML').strip()\n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, [job_title, company, location, url])\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, [job_title, company, location, url]])\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if jobs_scraped.shape == (4,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "        \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        descs = []\n",
    "        try:\n",
    "            # scraping seniority, emp_type, job_func, job_desc, posted ago\n",
    "            driver.get(job[3])\n",
    "            wait = WebDriverWait(driver, timeout=2)\n",
    "            ul = wait.until(EC.presence_of_element_located((By.XPATH, \".//ul[@class='description__job-criteria-list']\")))\n",
    "            desc_job = ul.get_attribute('outerHTML')\n",
    "            desc_job = BeautifulSoup(desc_job, 'html.parser')\n",
    "            desc_job = desc_job.find_all('span')\n",
    "            descs = [job[3]]\n",
    "            for i in [0,1,2]:\n",
    "                try:\n",
    "                    descs.append(desc_job[i].contents[0].replace('\\n', '').strip())\n",
    "                except:\n",
    "                    descs.append('')\n",
    "            try:\n",
    "                desc_gen = driver.find_element(By.XPATH, \"//div[@class='description__text description__text--rich']/section/div\")\n",
    "                desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "                descs.append(desc_gen)\n",
    "            except:\n",
    "                descs.append('')\n",
    "            try:\n",
    "                posted_ago = driver.find_element(By.XPATH, \"//span[@class='posted-time-ago__text topcard__flavor--metadata']\")\n",
    "                posted_ago = posted_ago.get_attribute('innerHTML').replace('\\n','').strip()\n",
    "                descs.append(posted_ago)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        except: \n",
    "            descs = ['','','','','','']\n",
    "            \n",
    "        if len(job_descs) == 0:\n",
    "            job_descs = np.append(job_descs, descs)\n",
    "        else:\n",
    "            job_descs = np.vstack([job_descs, descs])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # merging\n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title', 'company', 'location', 'link']\n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if job_descs_df.shape == (6,1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    \n",
    "    job_descs_df.columns = ['link','seniority','emp_type', 'job_function', 'job_desc', 'posted_ago']\n",
    "    linkedin_df = jobs_df.merge(job_descs_df, on='link')\n",
    "except:\n",
    "    print(\"LinkedIn not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    driver.close()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c28a1-cfb7-40e6-9b15-e2a2cf3bf3a2",
   "metadata": {},
   "source": [
    "# Scraping FoundIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13ae83b-2953-4c67-8f68-46665d22b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba34f8e9-0d28-4f33-951b-a3b8b8dd3d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mService(ChromeDriverManager()\u001b[38;5;241m.\u001b[39minstall()))\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.foundit.com.ph/search/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://www.foundit.com.ph/search/{job_query}-jobs\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    job_list = driver.find_elements(By.XPATH, \"//div[@class='srpResultCard']/div\")\n",
    "    job_list = job_list[1:] # remove the header\n",
    "    \n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        # scraping title, company, url\n",
    "        job_title = job.find_element(By.XPATH, \".//a[@title]\")\n",
    "        url = job_title.get_attribute('href')\n",
    "        job_title = job_title.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "        company = job.find_element(By.XPATH, \".//div[@class='companyName']/span\")\n",
    "        company = company.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "        descs = [job_title, company, url]\n",
    "    \n",
    "        # scraping location, seniority, posted ago\n",
    "        job.find_element(By.XPATH, './/div[@onclick]/div').click()\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        try:\n",
    "            desc_job = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobHighlight']/div/div/div\")))\n",
    "            for i in [0,1,2]:\n",
    "                if i != 2:\n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//div[@class='details']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                else: \n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//span[@class='btnHighighlights']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').split('</i>')\n",
    "                    detail = detail[1].strip()\n",
    "                descs.append(detail)\n",
    "        except:\n",
    "            descs.append('','','')\n",
    "    \n",
    "        # scraping emp_type, job function, general job_desc\n",
    "        try:\n",
    "            desc_job_2 = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobDetail']/div/div\")))\n",
    "            for i in [0,2]:\n",
    "                try:\n",
    "                    detail = desc_job_2[i].find_element(By.XPATH, \".//div[@class='jobDesc']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                    descs.append(detail)\n",
    "                except:\n",
    "                    descs.append('')\n",
    "        except:\n",
    "            descs.extend(['',''])\n",
    "        try:\n",
    "            desc_gen = wait.until(EC.visibility_of_element_located((By.XPATH, \".//p[@class='jobDescInfo']\")))\n",
    "            desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "            descs.append(desc_gen)\n",
    "        except:\n",
    "            descs.append('')\n",
    "            \n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, descs)\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "    \n",
    "    if jobs_scraped.shape == (9,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    foundit_df = pd.DataFrame(jobs_scraped)\n",
    "    foundit_df.columns = ['job_title','company','link','location','seniority','posted','emp_type','job_func','job_desc']\n",
    "except:\n",
    "    print(\"FoundIt not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    driver.close()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0b1aa-37be-46d8-88f3-f7d635d37958",
   "metadata": {},
   "source": [
    "# Scraping Kalibrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ab3c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "428415d6-27a1-4842-8ab6-4e9126ffce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOTAL TIME 201.09589552879333\n"
     ]
    }
   ],
   "source": [
    "# insert scraping code here\n",
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "min_jobs = 50\n",
    "url = f\"https://www.kalibrr.com/home/te/{job_query}\"\n",
    "kalibrr_df = ''\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    job_list = []\n",
    "    # load job list \n",
    "    while len(job_list) < min_jobs:\n",
    "        # click button to load more jobs until job_list > min_jobs\n",
    "        try:\n",
    "            # Re-evaluate job_list after each load\n",
    "            job_list = driver.find_elements(By.XPATH, \"//div[@itemscope]/div\")\n",
    "        \n",
    "            wait = WebDriverWait(driver, 5)\n",
    "            load_more = wait.until(EC.presence_of_element_located((By.XPATH, \"//button[@class='k-btn-primary']\")))\n",
    "            load_more.click()\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        descs = []\n",
    "        # scraping job title, company, emp_type, location\n",
    "        try:\n",
    "            job_title = job.find_element(By.XPATH, \".//h2[@data-tooltip-id='job-title-tooltip-[object Object]']/a\")\n",
    "            url = job_title.get_attribute('href')\n",
    "            job_title = job_title.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            descs.append(job_title)\n",
    "            descs.append(url)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            company = job.find_element(By.XPATH, \".//span[@class='k-inline-flex k-items-center k-mb-1']/a\")\n",
    "            company = company.text.replace('\\n', '').strip()\n",
    "            descs.append(company)\n",
    "        except:\n",
    "            descs.append('')\n",
    "            \n",
    "        try:\n",
    "            emp_type = job.find_element(By.XPATH, \"./div[@class='k-relative']/div/span/span[@class='k-text-gray-500']\")\n",
    "            emp_type = emp_type.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            descs.append(emp_type)\n",
    "        except:\n",
    "            descs.append('')\n",
    "    \n",
    "        try:\n",
    "            location = job.find_element(By.XPATH, \"./div[@class='k-relative']/div/span/span[@class='k-text-gray-500 k-block k-pointer-events-none']\")\n",
    "            location = location.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "            descs.append(location)\n",
    "        except:\n",
    "            descs.append('')\n",
    "    \n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, descs)\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "    \n",
    "    if jobs_scraped.shape == (5,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        descs = [job[1]]\n",
    "        try:\n",
    "            driver.get(job[1])\n",
    "            # scrape desc, job categ, job level\n",
    "            try:\n",
    "                wait = WebDriverWait(driver, timeout=2)\n",
    "                job_func = wait.until(EC.presence_of_element_located((By.XPATH, \".//div[@class='md:k-flex']//dt[contains(text(),'Job Category')]/following-sibling::dd/a\")))\n",
    "                job_func = job_func.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                descs.append(job_func)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        \n",
    "            try:\n",
    "                seniority = driver.find_element(By.XPATH, \".//div[@class='md:k-flex']//dt[contains(text(),'Job Level')]/following-sibling::dd/a\")\n",
    "                seniority = seniority.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                descs.append(seniority)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        \n",
    "            gen_desc = ''\n",
    "            try:\n",
    "                job_desc = driver.find_element(By.XPATH, \".//div[@itemprop='description']\")\n",
    "                job_desc = job_desc.get_attribute('innerHTML')\n",
    "                gen_desc += job_desc\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "            try:\n",
    "                job_qual = driver.find_element(By.XPATH, \".//div[@itemprop='qualifications']\")\n",
    "                job_qual = job_qual.get_attribute('innerHTML')\n",
    "                gen_desc += job_qual\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "            try:\n",
    "                job_benef = driver.find_element(By.XPATH, \".//div[@itemprop='jobBenefits']\")\n",
    "                job_benef = job_benef.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                gen_desc += job_qual\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "            try:\n",
    "                job_skills = driver.find_element(By.XPATH, \".//ul\")\n",
    "                job_skills = job_skills.get_attribute('innerHTML')\n",
    "                gen_desc += job_skills\n",
    "            except:\n",
    "                gen_desc = gen_desc\n",
    "        \n",
    "            descs.append(gen_desc)\n",
    "        \n",
    "            if len(job_descs) == 0:\n",
    "                job_descs = np.append(job_descs, descs)\n",
    "            else:\n",
    "                job_descs = np.vstack([job_descs, descs])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title', 'link' ,'company', 'emp_type', 'location']\n",
    "    \n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    if job_descs_df.shape == (4, 1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    job_descs_df.columns = ['link', 'job_func', 'seniority','gen_desc']\n",
    "    kalibrr_df = jobs_df.merge(job_descs_df, on='link', how='left')\n",
    "\n",
    "except:\n",
    "    print(\"Kalibrr not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    print()\n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "815492d3-9df1-43b3-b70c-ff335b3a4c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4992053-91cb-44d1-9897-735fc204099f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_descs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9a86-55f2-45c6-bb76-5a1aa55d8ae2",
   "metadata": {},
   "source": [
    "# Scraping Jobstreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "383425ff-6874-46dc-aa1d-699166db2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert scraping code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
