{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2db9e60-1c6a-40a8-8d5d-416368957326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e98529-f1b5-4d11-a080-e4912c60a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a2be0-f600-46ad-8b7c-b4246abaa039",
   "metadata": {},
   "source": [
    "# Scraping Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ac0e08-be70-4d8d-b574-ed4d1a5af5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462a4870-0a96-4333-9dcc-d9fa007700d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 241.68631982803345\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://ph.linkedin.com/jobs/{job_query}-jobs\"\n",
    "try:\n",
    "    driver.get(url)\n",
    "    ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "    job_list = driver.find_elements(By.XPATH, \"//ul[@class='jobs-search__results-list']/li\")\n",
    "    \n",
    "    # job catalog scraping\n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        card = job.find_element(By.XPATH, \".//a[@data-tracking-control-name='public_jobs_jserp-result_search-card']\")\n",
    "        url = card.get_attribute('href')\n",
    "        location = job.find_element(By.XPATH, \".//span[@class='job-search-card__location']\").get_attribute('innerHTML')\n",
    "        location = location.replace('\\n', '').strip()\n",
    "        job_title = job.find_element(By.XPATH, \".//h3[@class='base-search-card__title']\").get_attribute('innerHTML').strip()\n",
    "        try: \n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']/a\").get_attribute('innerHTML').strip()\n",
    "        except:\n",
    "            company = job.find_element(By.XPATH, \".//h4[@class='base-search-card__subtitle']\").get_attribute('innerHTML').strip()\n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, [job_title, company, location, url])\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, [job_title, company, location, url]])\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if jobs_scraped.shape == (4,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "        \n",
    "    # individual job scraping\n",
    "    job_descs = np.array([])\n",
    "    for job in jobs_scraped:\n",
    "        driver.get(job[3])\n",
    "        descs = []\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, timeout=2)\n",
    "            ul = wait.until(EC.presence_of_element_located((By.XPATH, \".//ul[@class='description__job-criteria-list']\")))\n",
    "            desc_job = ul.get_attribute('outerHTML')\n",
    "            desc_job = BeautifulSoup(desc_job, 'html.parser')\n",
    "            desc_job = desc_job.find_all('span')\n",
    "            descs = [job[3]]\n",
    "            for i in [0,1,2]:\n",
    "                try:\n",
    "                    descs.append(desc_job[i].contents[0].replace('\\n', '').strip())\n",
    "                except:\n",
    "                    descs.append('')\n",
    "            try:\n",
    "                desc_gen = driver.find_element(By.XPATH, \"//div[@class='description__text description__text--rich']/section/div\")\n",
    "                desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "                descs.append(desc_gen)\n",
    "            except:\n",
    "                descs.append('')\n",
    "        except: \n",
    "            descs = ['','','','','']\n",
    "            \n",
    "        if len(job_descs) == 0:\n",
    "            job_descs = np.append(job_descs, descs)\n",
    "        else:\n",
    "            job_descs = np.vstack([job_descs, descs])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # merging\n",
    "    jobs_df = pd.DataFrame(jobs_scraped)\n",
    "    jobs_df.columns = ['title', 'company', 'location', 'link']\n",
    "    job_descs_df = pd.DataFrame(job_descs)\n",
    "    \n",
    "    # if there is only 1 scraped job\n",
    "    if job_descs_df.shape == (5,1):\n",
    "        job_descs_df = job_descs_df.T\n",
    "    \n",
    "    job_descs_df.columns = ['link','seniority','emp_type', 'job_function', 'job_desc']\n",
    "    linkedin_df = jobs_df.merge(job_descs_df, on='link')\n",
    "except:\n",
    "    print(\"LinkedIn not accessible\")\n",
    "finally:\n",
    "    # close driver\n",
    "    driver.close()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c28a1-cfb7-40e6-9b15-e2a2cf3bf3a2",
   "metadata": {},
   "source": [
    "# Scraping FoundIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f13ae83b-2953-4c67-8f68-46665d22b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query = \"python-developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba34f8e9-0d28-4f33-951b-a3b8b8dd3d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn not accessible\n",
      "TOTAL TIME 194.24577140808105\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = f\"https://www.foundit.com.ph/search/{job_query}-jobs\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    job_list = driver.find_elements(By.XPATH, \"//div[@class='srpResultCard']/div\")\n",
    "    job_list = job_list[1:] # remove the header\n",
    "    \n",
    "    jobs_scraped = np.array([])\n",
    "    for job in job_list:\n",
    "        # scraping title, company, url\n",
    "        job_title = job.find_element(By.XPATH, \".//a[@title]\")\n",
    "        url = job_title.get_attribute('href')\n",
    "        job_title = job_title.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "        company = job.find_element(By.XPATH, \".//div[@class='companyName']/span\")\n",
    "        company = company.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "        descs = [job_title, company, url]\n",
    "    \n",
    "        # scraping location, seniority, posted ago\n",
    "        job.find_element(By.XPATH, './/div[@onclick]/div').click()\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        try:\n",
    "            desc_job = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobHighlight']/div/div/div\")))\n",
    "            for i in [0,1,2]:\n",
    "                if i != 2:\n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//div[@class='details']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                else: \n",
    "                    detail = desc_job[i].find_element(By.XPATH, \".//span[@class='btnHighighlights']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').split('</i>')\n",
    "                    detail = detail[1].strip()\n",
    "                descs.append(detail)\n",
    "        except:\n",
    "            descs.append('','','')\n",
    "    \n",
    "        # scraping emp_type, job function, general job_desc\n",
    "        try:\n",
    "            desc_job_2 = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \".//div[@id='jobDetail']/div/div\")))\n",
    "            for i in [0,2]:\n",
    "                try:\n",
    "                    detail = desc_job_2[i].find_element(By.XPATH, \".//div[@class='jobDesc']\")\n",
    "                    detail = detail.get_attribute('innerHTML').replace('\\n', '').strip()\n",
    "                    descs.append(detail)\n",
    "                except:\n",
    "                    descs.append('')\n",
    "        except:\n",
    "            descs.extend(['',''])\n",
    "        try:\n",
    "            desc_gen = wait.until(EC.visibility_of_element_located((By.XPATH, \".//p[@class='jobDescInfo']\")))\n",
    "            desc_gen = desc_gen.get_attribute('innerHTML')\n",
    "            descs.append(desc_gen)\n",
    "        except:\n",
    "            descs.append('')\n",
    "            \n",
    "        if len(jobs_scraped) == 0:\n",
    "            jobs_scraped = np.append(jobs_scraped, descs)\n",
    "        else:\n",
    "            jobs_scraped = np.vstack([jobs_scraped, descs])\n",
    "    \n",
    "    if jobs_scraped.shape == (9,):\n",
    "        jobs_scraped = np.array([list(jobs_scraped)])\n",
    "    \n",
    "    foundit_df = pd.DataFrame(jobs_scraped)\n",
    "    foundit_df.columns = ['job_title','company','link','location','seniority','posted','emp_type','job_func','job_desc']\n",
    "except:\n",
    "    print(\"FoundIt not accessible\")\n",
    "finally:\n",
    "    driver.close()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"TOTAL TIME\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "74a1f8db-cb4a-4f9a-878b-3309f8244fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d0b1aa-37be-46d8-88f3-f7d635d37958",
   "metadata": {},
   "source": [
    "# Scraping Kalibrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "428415d6-27a1-4842-8ab6-4e9126ffce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert scraping code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9a86-55f2-45c6-bb76-5a1aa55d8ae2",
   "metadata": {},
   "source": [
    "# Scraping Jobstreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "383425ff-6874-46dc-aa1d-699166db2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert scraping code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d3f59-3083-4531-8b06-95fc6538f939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
